
# Journal — 10-04-2025 — DAY 4

## 1) What I learned (bullets, not prose)
- Snowflake = normalized, less redundancy, slower for queries.
- Star = denormalized, more redundancy, faster for queries.
- DE practice: No update, No delete
- You can transform the data- just add another column, HOWEVER "Data is NOT meant to be understood, it is MEANT to be efficient”
- Problem framming first --> solution
- different business- different categorization 
- trade-off happen when you do not have a control on what is happening or what happened in the business.
- we do dimensional modelling(star schema & snow flakes) for a simple query
- trend = line; comparison = bar; proportion = pie/donut

## 2) New vocabulary (define in your own words)
- **term** — my definition
- trade-off - if you choose one thing, you might lose a bit of another.
- procurement- buying the things you need from other people or companies
- operational- day to day
- stategic- long term


## 3) Data Engineering mindset applied (what principles did I use?)
- STEP UP if no one does!

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Decision: Use the sales and employee-related tables to determine quarterly revenue by employee.
- Why: These tables contain transaction details (revenue) and the employee identifiers needed to answer the question.
- Alternatives: Could have used only the invoices table without joining employees, but that would not attribute revenue to specific staff.
- Trade-off: More joins mean slightly more complexity in the query, but this provides a more accurate view of revenue by employee.

## 5) Open questions (things I still don’t get)
- In cleaning data, can we omit a column from the raw data
- Do we need to master the creation  of vizualization? How to decide what vizualization is best to use , is it according to the business/analytics questions or according to type of data?
- What are the trade-offs in data engineering?
- Is the infrastruction of database can be interrupted?
- If the DPA is violated, do we need to remove that specific column from our raw data?

## 6) Next actions (small, doable steps)
- [ ]clean: hamonize, enforce types, fix categories - structure
- [ ]Mart: star schema, consistent grain, measures and dims- business
- [ ]Ensures data quality.

## 7) Artifacts & links (code, queries, dashboards)
- https://dataengineering.ph/
- dbt docs denerate
- dbt test (specific table)
- dbt build (run + test)
* schema.yml for ??/

---

### Mini reflection (3–5 sentences)
We use dlt because we want to create system; automation.
SLQ is for testing adhoc.
Manual SQL: tiny scope, quick, throwaway checks
dlt; team scale, durability, tests, docs, lineage, cl

combination: ad-hoc SQL for exploration. codify in dbt for production



### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](<img src="https://www.montecarlodata.com/wp-content/uploads/2023/11/data-pipelines-chaos.png" jsaction="" class="sFlh5c FyHeAf iPVvYb" style="max-width: 577px; height: 229px; margin: 0px; width: 305px;" alt="The Chaos Data Engineering Manifesto: Spare The Rod, Spoil Prod" jsname="kn3ccd"> "chaotic mind")
